{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alzheimer MRI 4-Class Detection using SAM as Feature Extractor\n",
    "\n",
    "This notebook uses the Segment Anything Model (SAM) ViT backbone as a feature extractor and a lightweight classifier for Alzheimer MRI classification. Optimized for 16GB RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\users\\shrir\\.conda\\envs\\tf_env\\lib\\site-packages (0.22.1+cpu)\n",
      "Requirement already satisfied: numpy in c:\\users\\shrir\\.conda\\envs\\tf_env\\lib\\site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: torch==2.7.1 in c:\\users\\shrir\\.conda\\envs\\tf_env\\lib\\site-packages (from torchvision) (2.7.1+cpu)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\shrir\\.conda\\envs\\tf_env\\lib\\site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\shrir\\.conda\\envs\\tf_env\\lib\\site-packages (from torch==2.7.1->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\shrir\\.conda\\envs\\tf_env\\lib\\site-packages (from torch==2.7.1->torchvision) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\shrir\\.conda\\envs\\tf_env\\lib\\site-packages (from torch==2.7.1->torchvision) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\shrir\\.conda\\envs\\tf_env\\lib\\site-packages (from torch==2.7.1->torchvision) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shrir\\.conda\\envs\\tf_env\\lib\\site-packages (from torch==2.7.1->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shrir\\.conda\\envs\\tf_env\\lib\\site-packages (from torch==2.7.1->torchvision) (2024.6.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\shrir\\.conda\\envs\\tf_env\\lib\\site-packages (from sympy>=1.13.3->torch==2.7.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shrir\\.conda\\envs\\tf_env\\lib\\site-packages (from jinja2->torch==2.7.1->torchvision) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "# --- Dataset Setup ---\n",
    "class_names = ['MildDemented', 'ModerateDemented', 'NonDemented', 'VeryMildDemented']\n",
    "class_to_idx = {name: i for i, name in enumerate(class_names)}\n",
    "\n",
    "class AlzheimerMRIDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.samples = []\n",
    "        for class_name in class_names:\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            for fname in os.listdir(class_dir):\n",
    "                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    self.samples.append((os.path.join(class_dir, fname), class_to_idx[class_name]))\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Data transforms\n",
    "# SAM models are trained on 1024x1024 images. Resizing smaller may lead to less optimal features\n",
    "# but is necessary for memory constraints. ViT-B's feature output is 256.\n",
    "img_size = 1024 # Keep 1024 as native SAM input, but be aware of memory\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Split dataset (simple split for demo)\n",
    "# IMPORTANT: Replace this path with the actual path to your dataset\n",
    "dataset = AlzheimerMRIDataset(r'C:\\\\Users\\\\shrir\\\\Music\\\\New folder\\\\Alzheimer_MRI_4_classes_dataset', transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Change num_workers to 0 for CPU to avoid multiprocessing issues\n",
    "# Reduced batch_size significantly due to memory constraints on 16GB RAM for 1024x1024 images\n",
    "batch_size = 1 # Start with batch size 1 or 2 due to severe memory limitations\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# --- Load SAM ViT Backbone as Feature Extractor ---\n",
    "sys.path.append('.')  # Ensure SAM code can be found if in current directory\n",
    "\n",
    "# IMPORTANT: You need to download sam_vit_b_01ec64.pth for this to work\n",
    "# It's a smaller model suitable for memory-constrained environments.\n",
    "# Download from: https://github.com/facebookresearch/segment-anything#model-checkpoints\n",
    "sam_checkpoint = r'C:\\Users\\shrir\\Music\\New folder\\sam_vit_b_01ec64.pth' # Updated path\n",
    "try:\n",
    "    from segment_anything import sam_model_registry\n",
    "    sam = sam_model_registry['vit_b'](checkpoint=sam_checkpoint) # Changed to 'vit_b'\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: SAM checkpoint '{sam_checkpoint}' not found.\")\n",
    "    print(\"Please ensure the file path is correct.\")\n",
    "    sys.exit(1) # Exit if SAM checkpoint is not found\n",
    "except ImportError:\n",
    "    print(\"Error: 'segment_anything' library not found.\")\n",
    "    print(\"Please install it: pip install git+https://github.com/facebookresearch/segment-anything.git\")\n",
    "    sys.exit(1)\n",
    "\n",
    "sam.eval()\n",
    "sam.to(device)\n",
    "\n",
    "# Freeze SAM parameters\n",
    "for param in sam.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Use only the image encoder as feature extractor\n",
    "def extract_features(images):\n",
    "    with torch.no_grad():\n",
    "        feats = sam.image_encoder(images)\n",
    "        # Global average pooling to get a fixed-size feature vector\n",
    "        pooled = feats.mean(dim=[2, 3])\n",
    "    return pooled\n",
    "\n",
    "# --- Classifier on top of SAM features ---\n",
    "# THIS CLASS DEFINITION MUST BE HERE BEFORE IT'S USED\n",
    "class SAMClassifier(nn.Module):\n",
    "    def __init__(self, feat_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Feature dimension for ViT-B is 256.\n",
    "feat_dim = 256\n",
    "num_classes = 4\n",
    "model = SAMClassifier(feat_dim, num_classes).to(device)\n",
    "\n",
    "# --- Training Loop ---\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 5  # Increase for better results\n",
    "print(f\"Starting training with batch_size={batch_size}, img_size={img_size}, feat_dim={feat_dim}\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        try:\n",
    "            feats = extract_features(images)\n",
    "            outputs = model(feats)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"RuntimeError during training batch: {e}\")\n",
    "            print(f\"Image shape: {images.shape}, Labels shape: {labels.shape}\")\n",
    "            continue # Skip this batch if an error occurs\n",
    "\n",
    "    if len(train_loader.dataset) > 0: # Avoid division by zero if dataset is empty\n",
    "        print(f'Train Loss: {running_loss/len(train_loader.dataset):.4f}')\n",
    "    else:\n",
    "        print('Train Loss: N/A (Empty training dataset)')\n",
    "\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            try:\n",
    "                feats = extract_features(images)\n",
    "                outputs = model(feats)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "            except RuntimeError as e:\n",
    "                print(f\"RuntimeError during validation batch: {e}\")\n",
    "                print(f\"Image shape: {images.shape}, Labels shape: {labels.shape}\")\n",
    "                continue # Skip this batch if an error occurs\n",
    "    if total > 0: # Avoid division by zero\n",
    "        print(f'Val Accuracy: {100*correct/total:.2f}%')\n",
    "    else:\n",
    "        print('Val Accuracy: N/A (Empty validation dataset)')\n",
    "\n",
    "# --- Inference Function ---\n",
    "def predict_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        feats = extract_features(image)\n",
    "        logits = model(feats)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        pred = torch.argmax(probs, dim=1).item()\n",
    "        confidence = probs[0, pred].item()\n",
    "    return class_names[pred], confidence\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "# try:\n",
    "#     # Make sure to replace with an actual image path from your dataset\n",
    "#     example_image_path = r'C:\\Users\\shrir\\Music\\New folder\\Alzheimer_MRI_4_classes_dataset\\NonDemented\\example.jpg'\n",
    "#     if os.path.exists(example_image_path):\n",
    "#         pred_class, conf = predict_image(example_image_path)\n",
    "#         print(f'Predicted: {pred_class} (Confidence: {conf:.2f})')\n",
    "#     else:\n",
    "#         print(f\"Example image not found at {example_image_path}. Please provide a valid path.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error during inference example: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "# --- Dataset Setup ---\n",
    "class_names = ['MildDemented', 'ModerateDemented', 'NonDemented', 'VeryMildDemented']\n",
    "class_to_idx = {name: i for i, name in enumerate(class_names)}\n",
    "\n",
    "class AlzheimerMRIDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.samples = []\n",
    "        for class_name in class_names:\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            for fname in os.listdir(class_dir):\n",
    "                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    self.samples.append((os.path.join(class_dir, fname), class_to_idx[class_name]))\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Data transforms\n",
    "# Data transforms\n",
    "img_size = 1024 # Changed from 224 to 1024\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Split dataset (simple split for demo)\n",
    "# IMPORTANT: Replace this path with the actual path to your dataset\n",
    "dataset = AlzheimerMRIDataset(r'C:\\\\Users\\\\shrir\\\\Music\\\\New folder\\\\Alzheimer_MRI_4_classes_dataset', transform=transform) \n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Change num_workers from 2 to 0 to address potential hanging issues, especially on Windows\n",
    "# Change batch_size to a smaller value\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0) # Changed from 16 to 2\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=0)   # Changed from 16 to 2\n",
    "\n",
    "# --- Load SAM ViT Backbone as Feature Extractor ---\n",
    "sys.path.append('.')  # Ensure SAM code can be found if in current directory\n",
    "\n",
    "# You need to have the SAM model code available (e.g., cloned from https://github.com/facebookresearch/segment-anything)\n",
    "# and the pre-trained SAM checkpoint file ('sam_vit_h_4b8939.pth') in the same directory or specified path.\n",
    "from segment_anything import sam_model_registry\n",
    "\n",
    "sam_checkpoint = 'sam_vit_h_4b8939.pth'\n",
    "try:\n",
    "    sam = sam_model_registry['vit_h'](checkpoint=sam_checkpoint)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: SAM checkpoint '{sam_checkpoint}' not found.\")\n",
    "    print(\"Please download it from https://github.com/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb (under 'Download the SAM checkpoint')\")\n",
    "    print(\"Or ensure 'segment_anything' library and the checkpoint are in the correct path.\")\n",
    "    sys.exit(1) # Exit if SAM checkpoint is not found\n",
    "\n",
    "sam.eval()\n",
    "sam.to(device)\n",
    "\n",
    "# Freeze SAM parameters\n",
    "for param in sam.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Use only the image encoder as feature extractor\n",
    "def extract_features(images):\n",
    "    with torch.no_grad():\n",
    "        feats = sam.image_encoder(images)\n",
    "        # Global average pooling to get a fixed-size feature vector\n",
    "        pooled = feats.mean(dim=[2, 3])\n",
    "    return pooled\n",
    "\n",
    "# --- Classifier on top of SAM features ---\n",
    "class SAMClassifier(nn.Module):\n",
    "    def __init__(self, feat_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "feat_dim = 1024  # For ViT-H\n",
    "num_classes = 4\n",
    "model = SAMClassifier(feat_dim, num_classes).to(device)\n",
    "\n",
    "# --- Training Loop ---\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 5  # Increase for better results\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        feats = extract_features(images)\n",
    "        outputs = model(feats)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    print(f'Train Loss: {running_loss/len(train_loader.dataset):.4f}')\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            feats = extract_features(images)\n",
    "            outputs = model(feats)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    print(f'Val Accuracy: {100*correct/total:.2f}%')\n",
    "\n",
    "# --- Inference Function ---\n",
    "def predict_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        feats = extract_features(image)\n",
    "        logits = model(feats)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        pred = torch.argmax(probs, dim=1).item()\n",
    "        confidence = probs[0, pred].item()\n",
    "    return class_names[pred], confidence\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "# pred_class, conf = predict_image('Alzheimer_MRI_4_classes_dataset/NonDemented/example.jpg')\n",
    "# print(f'Predicted: {pred_class} (Confidence: {conf:.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset Setup ---\n",
    "class_names = ['MildDemented', 'ModerateDemented', 'NonDemented', 'VeryMildDemented']\n",
    "class_to_idx = {name: i for i, name in enumerate(class_names)}\n",
    "\n",
    "class AlzheimerMRIDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.samples = []\n",
    "        for class_name in class_names:\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            for fname in os.listdir(class_dir):\n",
    "                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    self.samples.append((os.path.join(class_dir, fname), class_to_idx[class_name]))\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Data transforms\n",
    "img_size = 224\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Split dataset (simple split for demo)\n",
    "dataset = AlzheimerMRIDataset(r'C:\\\\Users\\\\shrir\\\\Music\\\\New folder\\\\Alzheimer_MRI_4_classes_dataset', transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Change num_workers from 2 to 0\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset Setup ---\n",
    "class_names = ['MildDemented', 'ModerateDemented', 'NonDemented', 'VeryMildDemented']\n",
    "class_to_idx = {name: i for i, name in enumerate(class_names)}\n",
    "\n",
    "class AlzheimerMRIDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.samples = []\n",
    "        for class_name in class_names:\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            for fname in os.listdir(class_dir):\n",
    "                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    self.samples.append((os.path.join(class_dir, fname), class_to_idx[class_name]))\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Data transforms\n",
    "img_size = 224\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Split dataset (simple split for demo)\n",
    "dataset = AlzheimerMRIDataset(r'C:\\Users\\shrir\\Music\\New folder\\Alzheimer_MRI_4_classes_dataset', transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load SAM ViT Backbone as Feature Extractor ---\n",
    "import sys\n",
    "sys.path.append('.')  # If SAM code is in the current directory\n",
    "\n",
    "# You need to have the SAM model code available, e.g. from https://github.com/facebookresearch/segment-anything\n",
    "from segment_anything import sam_model_registry\n",
    "\n",
    "sam_checkpoint = 'sam_vit_h_4b8939.pth'\n",
    "sam = sam_model_registry['vit_h'](checkpoint=sam_checkpoint)\n",
    "sam.eval()\n",
    "sam.to(device)\n",
    "\n",
    "# Freeze SAM parameters\n",
    "for param in sam.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Use only the image encoder as feature extractor\n",
    "def extract_features(images):\n",
    "    with torch.no_grad():\n",
    "        feats = sam.image_encoder(images)\n",
    "        # Global average pooling\n",
    "        pooled = feats.mean(dim=[2, 3])\n",
    "    return pooled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Classifier on top of SAM features ---\n",
    "class SAMClassifier(nn.Module):\n",
    "    def __init__(self, feat_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "feat_dim = 1024  # For ViT-H\n",
    "num_classes = 4\n",
    "model = SAMClassifier(feat_dim, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Loop ---\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 5  # Increase for better results\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        feats = extract_features(images)\n",
    "        outputs = model(feats)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    print(f'Train Loss: {running_loss/len(train_loader.dataset):.4f}')\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            feats = extract_features(images)\n",
    "            outputs = model(feats)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    print(f'Val Accuracy: {100*correct/total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inference Function ---\n",
    "def predict_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        feats = extract_features(image)\n",
    "        logits = model(feats)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        pred = torch.argmax(probs, dim=1).item()\n",
    "        confidence = probs[0, pred].item()\n",
    "    return class_names[pred], confidence\n",
    "\n",
    "# Example usage:\n",
    "# pred_class, conf = predict_image('Alzheimer_MRI_4_classes_dataset/NonDemented/example.jpg')\n",
    "# print(f'Predicted: {pred_class} (Confidence: {conf:.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
