{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alzheimerâ€™s Disease Detection: Fusion Model (CNN + SAM/ViT)\n",
    "\n",
    "- **Data:** `Data/` (class folders with images)\n",
    "- **SAM weights:** `sam_vit_h_4b8939.pth` (optional, will load if compatible)\n",
    "- **Image size:** 128x128\n",
    "- **Batch size:** 16\n",
    "- **Epochs:** 10\n",
    "- **Fusion:** Weighted average (CNN + SAM)\n",
    "- **Classifier:** MLP head for both branches\n",
    "- **Optimized for 16GB RAM, Python 3.11**\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install torch torchvision torchaudio matplotlib scikit-learn tqdm einops\n",
    "# !pip install opencv-python\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "import gc\n",
    "from einops import rearrange\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "DATA_DIR = 'Data'\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(DATA_DIR, transform=transform)\n",
    "class_names = dataset.classes\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Split dataset\n",
    "from torch.utils.data import random_split\n",
    "n_total = len(dataset)\n",
    "n_train = int(0.7 * n_total)\n",
    "n_val = int(0.15 * n_total)\n",
    "n_test = n_total - n_train - n_val\n",
    "train_ds, val_ds, test_ds = random_split(dataset, [n_train, n_val, n_test])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lightweight CNN Backbone + MLP Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * (IMG_SIZE//8) * (IMG_SIZE//8), 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.mlp(x)\n",
    "        return x\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Minimal ViT/SAM-like Model + MLP Classifier\n",
    "(This is a small, educational version for demonstration. For real SAM, use Meta's repo.)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=128, patch_size=16, in_chans=3, embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # (B, embed_dim, H/ps, W/ps)\n",
    "        x = x.flatten(2).transpose(1, 2)  # (B, n_patches, embed_dim)\n",
    "        return x\n",
    "\n",
    "class MiniViTBlock(nn.Module):\n",
    "    def __init__(self, dim, heads=4, mlp_dim=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(dim, heads, dropout=dropout, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class MiniSAM(nn.Module):\n",
    "    def __init__(self, num_classes, img_size=128, patch_size=16, embed_dim=64, depth=4):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, 3, embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, (img_size // patch_size) ** 2 + 1, embed_dim))\n",
    "        self.blocks = nn.Sequential(*[MiniViTBlock(embed_dim) for _ in range(depth)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        x = self.patch_embed(x)  # (B, n_patches, embed_dim)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x[:, 0])  # Use CLS token\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "\n",
    "# Try to load your weights if compatible\n",
    "sam_model = MiniSAM(num_classes)\n",
    "try:\n",
    "    sam_model.load_state_dict(torch.load('sam_vit_h_4b8939.pth', map_location='cpu'), strict=False)\n",
    "    print('Loaded SAM weights (partial or full)')\n",
    "except Exception as e:\n",
    "    print('Could not load SAM weights:', e)\n",
    "sam_model.eval()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fusion Model (Weighted Average, MLP heads)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class FusionModel(nn.Module):\n",
    "    def __init__(self, cnn, sam, num_classes, alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.cnn = cnn\n",
    "        self.sam = sam\n",
    "        self.alpha = alpha  # weight for CNN, (1-alpha) for SAM\n",
    "    def forward(self, x):\n",
    "        out_cnn = self.cnn(x)\n",
    "        out_sam = self.sam(x)\n",
    "        return self.alpha * out_cnn + (1 - self.alpha) * out_sam\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "cnn = SimpleCNN(num_classes).to(device)\n",
    "sam_model = sam_model.to(device)\n",
    "fusion = FusionModel(cnn, sam_model, num_classes, alpha=0.5).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(fusion.parameters(), lr=1e-3)\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in tqdm(loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "def eval_model(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_one_epoch(fusion, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc = eval_model(fusion, val_loader, criterion)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}')\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "test_loss, test_acc = eval_model(fusion, test_loader, criterion)\n",
    "print(f'Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}')\n",
    "\n",
    "# Classification report\n",
    "fusion.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = fusion(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names))\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Requirements\n",
    "\n",
    "```
    torch
    torchvision
    torchaudio
    matplotlib
    scikit-learn
    tqdm
    opencv-python
    einops
    ```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
